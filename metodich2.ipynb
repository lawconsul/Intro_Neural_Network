{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 3. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рабочее место пользователя KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Я постарался собрать основные пакеты, которые могут понадобиться при установке  KERAS на новом рабочем месте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda uninstall keras\n",
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........s...........................................x...........................s............................................x....................................s/Users/radik/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py:258: H5pyDeprecationWarning: File.fid has been deprecated. Use File.id instead.\n",
      "  \"Use File.id instead.\", H5pyDeprecationWarning)\n",
      "...s......ss.......................................................................................................ssssss..................................E..................................x....x.........................x......x..................................................ssss...................s........................................\n",
      "======================================================================\n",
      "ERROR: test_deprecation_available_ftypes (h5py.tests.old.test_h5t.TestDeprecation)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/radik/opt/anaconda3/lib/python3.7/site-packages/h5py/tests/old/test_h5t.py\", line 218, in test_deprecation_available_ftypes\n",
      "    with self.assertWarnsRegex(H5pyDeprecationWarning, warning_message) as warning:\n",
      "  File \"/Users/radik/opt/anaconda3/lib/python3.7/unittest/case.py\", line 230, in __enter__\n",
      "    for v in sys.modules.values():\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 509 tests in 4.758s\n",
      "\n",
      "FAILED (errors=1, skipped=17, expected failures=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=509 errors=1 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "h5py.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "pydot.graph_from_dot_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### К-сожалению, мне не удалось понять философию, того как правильно применять h5py и pydot. А других способов визуализации графов модели NN мне неизвестно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет  MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full neural network code!\n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) # - 0.5\n",
    "test_images = (test_images / 255)   #- 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(10, activation='relu', input_shape=(784,)),\n",
    "  Dense(20, activation='softmax'),\n",
    "  Dense(30, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger('training.log', separator='_',append=True)\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=500,\n",
    "  batch_size=32,\n",
    "  callbacks=[csv_logger],\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 25 test images.\n",
    "predictions = model.predict(test_images[:25])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:25]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 499/500\n",
    "60000/60000 [==============================] - 3s 43us/step - loss: 0.0616 - accuracy: 0.9813\n",
    "Epoch 500/500\n",
    "60000/60000 [==============================] - 3s 43us/step - loss: 0.0602 - accuracy: 0.9817\n",
    "10000/10000 [==============================] - 0s 34us/step\n",
    "[7 2 1 0 4 1 4 9 9 9 0 6 9 0 1 5 9 7 8 4 9 6 6 5 4]\n",
    "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model ver2.\n",
    "model = Sequential([\n",
    "  Dense(10, activation='relu', input_shape=(784,)),\n",
    "  Dense(30, activation='softmax'),\n",
    "  Dense(30, activation='softmax'),\n",
    "  Dense(30, activation='softmax'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "60000/60000 [==============================] - 3s 57us/step - loss: 0.1583 - accuracy: 0.9564\n",
    "10000/10000 [==============================] - 0s 33us/step \n",
    "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1]\n",
    "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод:\n",
    "### Я решил не гоняться за показателем accuracy, а пойти от упрощения.\n",
    "Модель из 2-х слоев по 10 элементов показывала хорошие результаты accuracy = 0.94. \n",
    "Но 8 и 19-й элемент корректно не распозновались: вместо 5 определялась 6 (9), вместо 3 - 8. \n",
    "### Усложнение модели за счет увеличение кол-ва эпох, слоев и элементов в них повысила accuracy на 6%, но 2 элемента по-прежнему идентифицируются неверно. \n",
    "Можно списать это на качество датасета, либо на предел NN на данной элементной базе; уверен, есть такая конфигурация при которой и эти элементы корректно распознаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет Boston Housing\n",
    "\n",
    "### Я решил применить NN для предсказания линейной регрессии в популярном датасете Boston Housing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=(x_train.shape[1]), init='normal', activation='relu'))\n",
    "    model.add(Dense(20, init='normal', activation='linear'))  # ten neuron, linear activation\n",
    "    model.add(Dense(1, init='normal', activation='linear'))  # one neuron, linear activation\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='adam')  # mse loss\n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=5000, batch_size=20, verbose=2)  # KerasRegressor for regression problem\n",
    "estimator.fit(x_test, y_test)\n",
    "\n",
    "csv_logger = CSVLogger('boston_housing.log', separator='_',append=True)\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('boston_housing.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = estimator.predict(x_test[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(predictions) # \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(y_test[:5]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 4999/5000\n",
    " - 0s - loss: 2.9452\n",
    "Epoch 5000/5000\n",
    " - 0s - loss: 3.6982\n",
    "[ 4.7438025 15.9756    19.063217  28.179684  21.890736 ]\n",
    "[ 7.2       18.8       19.        27.        22.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод:\n",
    "### для получения устраивающего меня результата пришлось:\n",
    "1. отказаться от метрики accuracy в сторону mse, т.к. из-за ее применения при работе с ЛР возникают проблемы\n",
    "2. переключиться с KerasClassifier на KerasRegressor, т.к. мы имеем дело с ЛР. Хотя результат был, но очень низкий\n",
    "3. использовать метод активации linear на внутренних слоях, т.к. он предназначен для применения в ЛР, ReLU - здесь не место. А также я добавил большее кол-во слоев, но не в ущерб временным затратам и используемой ОЗУ (на моем ноутбуке всего 8 ГБ ОЗУ и 4 потока)\n",
    "4. для обучения модели понадобилось на порядок больше эпох, но итоговый результат того стоил. Кажется логичным, что для ЛР требуется более глубокое обучение, по сравнению с обычной классификацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIPS & TRICKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVLogger\n",
    "#### Мне всегда удобно работать с внешним лог-файлам. Поэтому этот фреймворк не стал для меня исключением "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    "  callbacks=[filename=csv_logger, separator='+', append=True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
